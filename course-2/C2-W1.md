# **Introduction to Building Data Pipelines in MLOps**

**Overview:**
- Second course in the Machine Learning Engineering for Production, and MLOps Specialization.
- First course covered the entire machine learning project lifecycle, including data topics and challenges in data pipelines.

**Instructor:**
- Instructor: Robert Growe, TensorFlow developer engineer at Google, an MLOps expert.

**Course Focus: Building Data Pipelines:**
- Dive deeper into data-related aspects of MLOps.
- Explore the data journey across a production system's lifecycle.

**Key Topics:**
1. **Data Processing with TFX Framework:**
   - Implement feature engineering, transformation, and selection using TFX framework.
   - Emphasis on structured and unstructured data types.

2. **Responsible Data Collection:**
   - Focus on building fair ML production systems.
   - Understand the importance of responsible data collection.

3. **Deep Model Performance Analysis:**
   - Gain insights into deep model performance analysis.
   - Understand the significance of understanding model performance at a deeper level.

4. **Improving Data Quality:**
   - Ways to enhance data quality for better training and generalization.
   - Address class imbalances in the data.

5. **TFX Library Usage:**
   - Become familiar with TFX library.
   - Learn to create a healthy and fully operational data pipeline.

6. **Addressing Evolving Data:**
   - Use ML metadata and enterprise schemas to handle quickly evolving data.

**Business Impact:**
- Highlight the importance of production ML in business.
- Stress the significance of treating people fairly and serving diverse customers well.

**Career Relevance:**
- Skills and tools covered are increasingly prominent in the current job market.

**Specialized Scenarios:**
- Brief mention of specialized scenarios like anomaly detection.

**Conclusion:**
- Acknowledgment that machine learning heavily relies on data.
- Emphasis on the critical impact of data quality on the success of a product or service.

**Next Steps:**
- Dive into the specifics of building effective data pipelines in the subsequent lectures.

# **Collecting, Labeling, and Validating Data in Production Machine Learning**

**Introduction:**
- Welcome to the lecture on collecting, labeling, and validating data in production machine learning.
- Focus on the importance of data in a production ML setting.

**Importance of Data in Production Environments:**
- Quotes from ML practitioners at Uber and Gojek emphasize the critical role of data in ML.
- Broken data is a common cause of issues in production ML systems.
- Improving data quality has a high return on investment in the machine learning lifecycle.

**Overview of Machine Learning in Production:**
- Production ML is a combination of ML development and modern software development practices.
- Challenges unique to production ML, which goes beyond modeling.
- Model constitutes about 5 percent; other aspects include deployment, maintenance, and improvement.

**Differences Between Academic and Production ML:**
- Academic: Static dataset; Production: Dynamic and shifting real-world data.
- Academic: Highest accuracy over the entire training set; Production: Emphasis on fast inference, interpretability, accuracy, and cost.
- Continuous monitoring, assessment, and retraining required in production.
- Interpretability and fairness crucial for production ML.

**Machine Learning Development:**
- Focus on data-related issues and prediction quality.
- Supervised learning requires accurate labels and a training dataset covering the same feature space as model requests.
- Dimensionality reduction for system optimization.
- Consideration of fairness, especially for rare conditions.

**Production ML Application Challenges:**
- Putting software into production requires expertise in both ML and modern software development.
- System design considerations: scalability, extension, configuration, consistency, security, and modularity

# **Introduction to ML Pipelines and TFX**

**Key Points:**

1. **ML Pipelines in Production:**
   - ML Pipelines are crucial for Production ML systems, serving as the backbone.
   - They automate, monitor, and maintain the ML workflow from data to a trained model.

2. **MLOps and Pipeline Orchestrators:**
   - Introduces the concept of MLOps, emphasizing the role of pipeline orchestrators.
   - Pipeline orchestrators, like Argo, Airflow, Celery, Luigi, and Kubeflow, schedule ML tasks based on dependencies in a Directed Acyclic Graph (DAG).

3. **Components of ML Pipelines:**
   - ML Pipelines typically follow the ML development process, from data ingestion to a trained model.
   - They are often represented as DAGs, encapsulating and formalizing the ML workflow.

4. **TFX - TensorFlow Extended:**
   - TFX is an open-source end-to-end Machine Learning platform used at Google.
   - TFX Pipelines consist of scalable components handling data ingestion, validation, feature engineering, training, validation, and prediction serving.

5. **TFX Components:**
   - TFX components include ExampleGen (data ingestion), StatisticsGen (data statistics), ExampleValidator (data validation), SchemaGen (schema generation), Transform (feature engineering), Tuner, Trainer, Evaluator, InfraValidator, and Pusher.

6. **Hello World of TFX:**
   - Walkthrough of a basic TFX Pipeline, starting with data ingestion using ExampleGen.
   - Components perform tasks such as generating statistics, validating examples, generating a schema, feature engineering, model training, hyperparameter tuning, model evaluation, and infrastructure validation.

7. **Production Deployment with Pusher:**
   - If the model outperforms the existing one, the Pusher component deploys it to production.
   - Production deployment options include TensorFlow Hub, TensorFlow JS, TensorFlow Lite, and TensorFlow Serving.

8. **TFX in the Course:**
   - TFX is utilized in the course to implement real ML Pipelines, combining open-source libraries like TensorFlow Data Validation, TensorFlow Transform, and TensorFlow Model Analysis.

9. **Key Takeaways:**
   - Production ML Pipelines extend beyond ML code, involving ML development and software development.
   - TFX provides an open-sourced end-to-end ML platform for scalable and maintainable ML workflows.

*Note: ML - Machine Learning, DAG - Directed Acyclic Graph, MLOps - Machine Learning Operations.*

# **Data Collection and Importance in Machine Learning**

**Key Points:**

1. **Introduction:**
   - To engage in machine learning, data is a prerequisite.
   - Production ML often involves finding ways to collect data, a crucial aspect discussed in this section.

2. **Real-world Example:**
   - Shared a personal experience involving creating a model to predict airport security checkpoint times.
   - Emphasized the challenges of collecting data in real-world scenarios, addressing security and logistical concerns.

3. **Data Quality's Crucial Role:**
   - Highlighted the critical role of data quality in ML model performance.
   - The quality of the model is only as good as the quality of the data.

4. **Data Pipelines in ML:**
   - Emphasized the use of data pipelines for automation in ML processes.
   - Described the sequence of automated tasks, including data collection, ingestion, preparation, and monitoring.

5. **Continuous Data Collection:**
   - Stressed that data collection is an ongoing process in most applications.
   - Underlined the importance of monitoring data quality throughout the application's lifespan.

6. **Quotes and Perspectives:**
   - Cited quotes from Uber and Gojek about the significance of data in ML production systems.
   - Defined ML data as a "first-class citizen" in the realm of programming languages.

7. **Data Optimization:**
   - In software 2.0, where goals and behavior are specified, data quality becomes a critical factor for success.
   - Data is compared to code in a software application, with optimization extending beyond just performance to maintainability and scalability.

8. **Models Aren't Magic:**
   - Cautioned that models are not magical and the success of a model depends on the quality and relevance of the data it is trained on.
   - Emphasized the need to remove non-predictive features and ensure training data covers the same feature space as prediction requests in production.

9. **Garbage In, Garbage Out:**
   - Underlined the importance of data quality in determining the quality of the model and the overall application.
   - The ease of measuring ML system performance distinguishes it from other software applications.

10. **Considerations in Data Collection:**
    - Outlined the considerations in data collection, including scalability, serving predictions, handling downtime, and accounting for factors like seasonality and trends.
    - Stressed the need for automation, testability, and maintainability throughout the ML development process.

11. **Understanding Users' Needs:**
    - Encouraged understanding users' needs and translating them into data problems.
    - Advised against blindly using a model that may not meet user requirements.

12. **Continuous Monitoring and Responsibility:**
    - Emphasized the need for continuous monitoring of data quality throughout the application's life.
    - Encouraged responsible data sourcing, considering issues of bias and fairness, topics to be covered later in the course.

*Note: ML - Machine Learning.*


# **Data Collection Strategy and Application Example**

**Key Points:**

1. **Introduction:**
   - Discussing the importance of data collection and using an example application: suggesting runs to runners based on their fitness levels.

2. **Understanding User Needs:**
   - The system aims to understand users' behaviors and preferences to suggest personalized runs.
   - Goal: Improve the consistency of running, ensuring runners complete runs and find them enjoyable.

3. **Key Considerations:**
   - **Data Quality:**
      - Considerations include the type and volume of data needed.
      - Questions about data annotation, frequency of updates, and anticipation of changes.
   - **User Needs to Data Needs:**
      - Translating user needs into specific data requirements is crucial.
      - Initial focus on understanding the user to avoid collecting irrelevant or "garbage" data.

4. **Example Dataset:**
   - Dataset includes three run examples (Boston Marathon, Seattle Oktoberfest 5K, Houston Half-marathon).
   - Features: Run type, Runner Time, Elevation.
   - Labels: Runner's rating of the fun level for each run.

5. **Get to Know Your Data:**
   - Identifying data sources and ensuring ongoing data collection.
   - Checking for consistency, outliers, errors, and variations in data encoding.
   - Addressing potential issues with missing values and encoding choices.

6. **Monitoring Data Sources:**
   - Early detection of errors and issues crucial for effective ML system operation.
   - Preparing for system issues and outages, considering scenarios like app downtime affecting recorded runner times.

7. **Measuring Data Effectiveness:**
   - Intuition about data value is essential but can be misleading.
   - Feature selection and engineering critical for maximizing predictive signals.
   - Continuous testing and evaluation to ensure the data is providing valuable information.

8. **User Demographics and Features:**
   - Obtaining user demographic data and GPS information.
   - Translating user characteristics into features: time of day, run duration, pace, distance, elevation, and potential sensor data.

9. **Labels and User Feedback:**
   - Labels: Focusing on "Runner Acceptance" - whether runners accept suggested runs.
   - User-generated feedback: Structured feedback for training the model, including user ratings of recommended runs.

10. **Key Takeaways:**
    - Understanding the user/application is paramount.
    - Translate user needs into well-defined features that offer predictive information.
    - Consider available data, details, and issues (e.g., sensor reliability).
    - Identify where the predictive information lies in the data.
    - Define correct labels aligned with the goals of the model.
    - Metrics: Decide on the metrics for evaluating model performance.

*Note: ML - Machine Learning.*

# **Responsible Data Collection: Security, Privacy, and Fairness**

**Key Points:**

1. **Introduction:**
   - Addressing responsible data collection, emphasizing security, privacy, and fairness.

2. **Example of Bias:**
   - Illustrating bias in an image classifier mislabeling images from diverse wedding traditions.
   - Highlighting the importance of addressing bias in data sets to prevent representational harm.

3. **Data Sources and Security:**
   - Different sources for ML data, including synthetic data, web scraping, and live data.
   - Emphasizing data security policies to protect Personally Identifiable Information (PII).

4. **Data Privacy:**
   - Managing data responsibly and respecting user privacy.
   - Users should control what data is collected, and mechanisms should prevent inadvertent data exposure.

5. **Privacy Measures:**
   - Techniques like aggregation and anonymization to protect PII.
   - Consideration of laws and regulations (e.g., GDPR) related to user privacy.

6. **ML System Failures and Fairness:**
   - Balancing fairness, accuracy, transparency, and explainability in ML systems.

7. **Ways ML Systems Can Fail:**
   - Examples of failures, including representational harm, opportunity denial, disproportionate product failure, and harm by disadvantage.

8. **Commitment to Fairness:**
   - The importance of committing to fairness from the beginning of the ML system development.
   - Ensuring equal experiences across demographic groups, avoiding consistent problematic predictions.

9. **Addressing Bias:**
   - Data labeled by humans may reflect biases; diversifying the user base can help.
   - Deploying fair models and considering biases arising from disproportionate representation.

10. **Reducing Bias in Data:**
    - Accurate labels are crucial; biases can arise from skewed data representation.
    - Reducing bias through diverse and representative data sources.

11. **Labeling Data:**
    - Labels come from automated systems or human raters.
    - Human raters may be generalists or subject matter experts depending on the complexity of the data.

12. **Types of Labelers:**
    - Generalists for straightforward labeling tasks.
    - Subject matter experts for complex tasks like medical diagnosis.
    - Users can provide valuable feedback in some applications.

13. **Key Considerations:**
    - Ensuring fair labelers and representation in the dataset.
    - Considering the incentives of labelers and controlling for potential biases.
    - Balancing cost considerations while maintaining data quality.
    - Recognizing the need for data freshness and periodic updates.

*Note: PII - Personally Identifiable Information, GDPR - General Data Protection Regulation.*

# **Adapting to Change in Production ML**

**Key Points:**

1. **Introduction:**
   - Heraclitus' quote on change being the only constant, emphasizing its relevance in production ML.

2. **Case Study: Shoe Retailer:**
   - Scenario of an online retailer using an ML model to predict click-through rates for shoe inventory.
   - Highlighting the challenge of detecting performance issues, particularly in a specific inventory segment like men's dress shoes.

3. **Detecting Problems Early:**
   - Importance of early detection of model performance issues.
   - Addressing potential causes and having systems in place for remediation.

4. **Types of Problems:**
   - Distinguishing between slow problems (gradual changes) and fast problems (abrupt changes within the system).
   - Monitoring for both types and considering remediation strategies.

5. **Gradual Problems:**
   - Trends and seasonality as gradual changes.
   - Distribution of features and their changing importance can impact model accuracy over time.

6. **Constant Changes in the World:**
   - The dynamic nature of the world affecting production settings.
   - Examples include changing styles, evolving processes, new competitors, business expansions, and fluctuating prices.

7. **Sudden Problems:**
   - Common sudden issues like data collection problems, changes in log formats, sensor movement, software updates, network connectivity issues, system failures, and credential problems.

8. **Understanding Model Sensitivity:**
   - Importance of understanding how the model reacts to different changes in the world.
   - Recognizing that mispredictions have varying impacts on business.

9. **Challenges with Available Data:**
   - Acknowledging that the collected data is often not ideal, and compromises may be necessary.
   - The model's objective as a proxy for the actual target, with click-through rates as a proxy for inventory ordering decisions.

10. **Managing Customer Experiences:**
    - Recognizing that a percentage of customers may have a negative experience due to mispredictions.
    - The goal is to minimize this percentage and identify affected customers for targeted improvements.

11. **The Inevitability of Change:**
    - Emphasizing that the real world is dynamic, and change is constant.
    - Concluding with the recurring theme that adaptation is crucial in the ever-changing landscape of production ML.

*Note: ML - Machine Learning.*

# **Adapting to Changing Ground Truth in Production ML**

**Key Points:**

1. **Introduction:**
   - Focusing on detecting problems in deployed models, particularly addressing data change and concept change or world change.
   - Differentiating between easy, harder, and really hard problems related to changing ground truth.

2. **Monitoring Models:**
   - Emphasizing the need to monitor models and validate results to find problems early.
   - Acknowledging the fundamental issue of changing ground truth and the necessity of labeling new data throughout the application's life.

3. **Easy Problems:**
   - Examples, like recognizing images of cats and dogs, where ground truth changes slowly.
   - Model retraining is primarily driven by improvements, and labeling is straightforward, often utilizing curated datasets or crowd-based sources.

4. **Harder Problems:**
   - Highlighting domains where ground truth changes faster, such as styles that evolve over weeks.
   - Model retraining driven by declining performance, and labeling may involve direct feedback or crowd-based human labeling.

5. **Really Hard Problems:**
   - Discussing domains with rapidly changing ground truth, like markets that shift in days or hours.
   - Identifying declining model performance as a key driver for retraining, while acknowledging challenges in labeling, suggesting weak supervision, and highlighting high-value domains.

6. **Model Performance Decay:**
   - Recognizing that model performance decays over time, whether slowly (e.g., cats and dogs) or rapidly (e.g., markets).
   - Highlighting the role of model retraining in improving or maintaining performance.

7. **Data Labeling Challenges:**
   - Emphasizing the importance of data labeling, particularly in supervised learning.
   - Encouraging tailored approaches based on the problem domain and available systems.

8. **Key Takeaways:**
   - Model performance decay is a natural occurrence, requiring proactive measures like retraining.
   - Data labeling strategies should be aligned with the specific problem domain and the pace of ground truth changes.

*Note: ML - Machine Learning.*

# **Methods of Data Labeling in Production ML**

**Key Points:**

1. **Introduction:**
   - Highlighting the focus on two common methods of data labeling: process feedback (direct labeling) and human labeling.
   - Teasing advanced methods like semi-supervised labeling, active learning, and weak supervision for future discussions.

2. **Need for Labels in Supervised Learning:**
   - Emphasizing the essential role of labels in supervised learning for utilizing data effectively.
   - Recognizing that model retraining is necessary based on the problem domain and its dynamics.

3. **Process Feedback or Direct Labeling:**
   - Defining process feedback as a continuous method of creating new training data for model retraining.
   - Explaining the process of deriving labels from system feedback, requiring solutions for time gaps between inference requests and feedback.
   - Drawing parallels with reinforcement learning, where labels are applied based on predictions.
   - Acknowledging advantages such as strong signals from click-through rates but noting that it may not be feasible in many domains.
   - Recommending log analysis tools like Logstash, Fluentd, and cloud-based tools for efficient log data management.

4. **Human Labeling:**
   - Describing human labeling as the assignment of labels by human raters to examine data.
   - Outlining the process, starting with raw data, recruiting human raters, providing instructions, and resolving conflicts.
   - Identifying advantages such as the generation of necessary labels for supervised learning.
   - Addressing disadvantages, including potential disagreements among raters, slowness, high costs, and the challenge of handling high-dimensional data.
   - Recognizing the limitations, like slow recruitment and potential expenses when dealing with specialists.

5. **Considerations for Data Labeling:**
   - Stressing the importance of considering the nature of the problem, the data, retraining frequency, required data volume, and associated costs when choosing a labeling method.
   - Acknowledging that both process feedback and human labeling have advantages and disadvantages.

6. **Key Takeaways:**
   - Different methods exist for data labeling, each with its own set of advantages and challenges.
   - The choice between process feedback and human labeling depends on the problem domain, data characteristics, and practical considerations.

*Note: ML - Machine Learning.*