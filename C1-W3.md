Week-3
**Title: Week 3 - Data in Machine Learning Operations (MLOps) Specialization**

- **Introduction:**
  - Final week of the course, focusing on data.
  - Importance of setting up data for training and modeling success.

- **Challenges in Defining Data:**
  - Example of detecting Iguanas to illustrate challenges.
  - Different labeling conventions by diligent labelers.
  - Emphasizes the need for label consistency.

- **Labeling Ambiguity:**
  - Phone defect detection example.
  - Varying opinions on significant defects.
  - Highlight on inconsistent labeling and its impact on learning algorithms.

- **Objective of the Week:**
  - Dive into best practices for the data stage in the machine learning project cycle.
  - Define data components (x and y), establish baselines, and organize data effectively.

- **Data Quality Impact:**
  - Preparation of datasets influences machine learning project success.
  - Importance of well-prepared data sets.

- **Downloading vs. Preparing Data:**
  - Acknowledges the common practice of downloading data for experimentation.
  - Emphasizes the significant impact of data preparation on project success.

- **Upcoming Focus:**
  - Introduction to upcoming examples of ambiguous data.
  - Preparation for techniques to improve data quality.

- **Conclusion:**
  - Setting the stage for exploring data quality improvement techniques in the next video.

**Note:** Understanding the challenges of labeling ambiguity and the impact of inconsistent labeling, the lecture emphasizes the importance of preparing datasets effectively for successful machine learning projects.

**Title: Understanding Types of Machine Learning Projects**

**Andrew Ng's Lecture Notes: MLOps Specialization Course**

**Introduction:**
- Framework for categorizing machine learning projects based on key factors.

**Two-Axis Categorization:**
- Two axes for categorization: Unstructured Data vs. Structured Data, and Small Data vs. Big Data.
- Unstructured data includes images, audio, and text; humans excel in processing.
- Structured data involves database records, less intuitive for humans.

**Data Size Threshold:**
- Threshold for small and large datasets set at 10,000 examples.
- Transitions are gradual, with emphasis on the manageability of examining examples manually.

**Examples:**
1. **Manufacturing Visual Inspection:**
   - Unstructured data (images of smartphones).
   - Small dataset (100 examples).
   - Data augmentation can help by generating more images.

2. **Predicting Housing Prices:**
   - Structured data (real value features of houses).
   - Small dataset (52 examples).

3. **Speech Recognition:**
   - Unstructured data (audio).
   - Large dataset (50 million examples).
   - Data augmentation useful for synthesizing audio clips.

4. **Product Recommendations:**
   - Structured data (user database).
   - Large dataset (million users).
   - Challenges in obtaining more data and using data augmentation.

**Best Practices for Unstructured Data:**
- Data augmentation is valuable (e.g., for image or speech data).
- Human labeling and synthesizing new data enhance performance.

**Best Practices for Structured Data:**
- Challenges in obtaining more data.
- Data augmentation is harder.
- Human labeling may be challenging due to limited examples.

**Data Size Considerations:**
- Small datasets require clean labels; inconsistencies impact a significant portion.
- Large datasets emphasize data processes; human review may be impractical.

**Data Size and Data Processes:**
- Small datasets benefit from manual review and consistent labeling.
- Large datasets require robust data processes due to the scale of labeling.

**Generalization and Hiring Insights:**
- Categorization aids in predicting generalization of data processes and ML ideas.
- Hiring individuals with experience in the same quadrant is advantageous.

**Adapting Advice:**
- Caution against one-size-fits-all advice in machine learning.
- Diverse problems may require different approaches; seek advice from the relevant quadrant.

**Conclusion:**
- Understanding the categorization of machine learning problems aids in tailored solutions.
- Importance of clean data, especially in small data problems.
- Next video: Exploring why clean data is crucial in small data problems.


**Title: Handling Small Datasets and Label Consistency**

**Introduction:**
- Addressing challenges in small datasets and the importance of clean, consistent labels.

**Small Datasets and Noisy Labels:**
- Small datasets, especially with noisy labels, pose challenges in confidently fitting a function.
- Illustration using a helicopter rotor speed prediction problem with only five examples.
- Difficulty in determining the mapping function from voltage to rotor speed with a small, noisy dataset.

**Learning from Large Datasets:**
- Contrast with large datasets where algorithms can average over noise, enabling more confident function fitting.
- Caution against neglecting practices for small datasets prevalent in large consumer Internet companies.

**Clean Labels in Small Datasets:**
- Emphasis on the significance of clean and consistent labels in small datasets.
- Example of training computer vision systems with just 30 images, highlighting the importance of clean labels.

**Phone Defect Inspection Example:**
- Discussing phone defect inspection and the impact of unclear labeling instructions on label inconsistency.
- Proposal to improve label consistency by having inspectors agree on scratch size thresholds.
- Importance of defining tasks, like bounding boxes around defects, to enhance image labeling consistency.

**Label Consistency Enhances Accuracy:**
- Demonstrating how label consistency leads to higher accuracy even with a limited dataset.
- Importance of inspectors agreeing on criteria for defects, facilitating more consistent image labeling.

**Small Data Challenges in Big Data Problems:**
- Acknowledgment that big data problems can still have small data challenges, particularly with rare events.
- Examples include rare web search queries, critical events in self-driving cars, and long-tail items in product recommendation systems.
- Highlighting the need for label consistency even in large datasets where rare events have limited examples.

**Conclusion:**
- Reinforcing the critical role of label consistency in small datasets and its ongoing importance in big data scenarios.
- Teaser for the next video focusing on concrete ideas and best practices for improving data label consistency.

**Title: Improving Label Consistency in Machine Learning Datasets**

1. **Process for Label Consistency:**
   - If concerned about inconsistent labels, have multiple labelers label the same examples.
   - Consider having the same labeler re-label an example after a break to check for self-consistency.
   - Address disagreements by involving responsible individuals like machine label engineers or subject matter experts.
   - Discuss and document a more consistent definition of the label (y) and update labeling instructions.

2. **Adjusting Input Information (x):**
   - If labelers find input (x) lacking information, consider modifying the input, such as improving lighting for images.
   - Iterative process: After improving x or label instructions, ask the team to label more data.

3. **Standardizing Definitions:**
   - Common outcome is standardizing label definitions to enhance data consistency.
   - Example: Standardizing the convention for labeling audio clips.

4. **Merging Classes:**
   - In cases of unclear distinctions, merge classes to simplify the task for learning algorithms.
   - Example: Merging classes for deep and shallow scratches on a phone surface.

5. **Creating a New Class for Uncertainty:**
   - Introduce a new class or label to capture uncertainty when labelers struggle to agree.
   - Example: Creating a borderline class for ambiguous cases in defect labeling.

6. **Speech Recognition Example:**
   - Utilize the example of unintelligible speech to introduce a new tag (unintelligible) for ambiguous cases.
   - Enhances labeling consistency when the audio clip is genuinely unclear.

7. **Working with Small vs. Big Datasets:**
   - For small datasets, resolve inconsistencies through discussions with labelers.
   - For big datasets, define consistent labels with a small group before instructing a larger group of labelers.

8. **Voting Mechanism (Consensus Labeling):**
   - Overused technique: Having multiple labelers vote to resolve inconsistencies.
   - Prefer addressing inconsistent instructions first to make individual labeler choices less noisy.

9. **Gaps in the ML World:**
   - Acknowledges the need for tools and ML Ops tools to enhance the consistency and repeatability of label improvement processes.
   - Calls for the development of tools to detect label inconsistencies and facilitate data quality improvement.

10. **Human Level Performance:**
    - Raises the question of human level performance on the task.
    - Highlights the importance and sometimes misuse of human level performance in machine learning.

11. **Looking Forward:**
    - Expresses anticipation for community efforts in developing tools to detect and improve label inconsistencies consistently.
    - Emphasizes the importance of addressing gaps in current ML practices for enhancing label quality.
**Title: Understanding Human Level Performance in Machine Learning**

**Key Points:**
1. **Introduction to Human Level Performance (HLP):**
   - HLP serves as a reference baseline for predicting inherently ambiguous outputs in certain machine learning tasks.
   - Sometimes misused, especially when dealing with unstructured data tasks.

2. **Estimating Irreducible Error:**
   - HLP is crucial for estimating Bayes error or irreducible error, particularly in unstructured data tasks.
   - Aids in analysis, prioritization, and establishing the potential of a given task.

3. **Example of Visual Inspection Tasks:**
   - Illustration using a visual inspection task to demonstrate the practical application of HLP.
   - Highlights the importance of understanding ground truth labels.

4. **Academic Benchmarking:**
   - In academia, HLP serves as a benchmark for research significance.
   - Beating HLP often contributes to establishing the academic significance of machine learning work.

5. **Caution Against Misuse of HLP:**
   - Cautionary note against using HLP to assert machine superiority over humans in practical applications.
   - Highlights the limitations, especially in scenarios where applications require more than just high accuracy.

6. **Inconsistent Labeling Instructions Issue:**
   - Discusses the problem of inconsistent labeling instructions affecting HLP measurements.
   - Demonstrates how a learning algorithm may gain an unfair advantage due to such inconsistencies.

7. **Unfair Advantage Scenario:**
   - Explains the scenario where a learning algorithm gains an advantage by consistently choosing one labeling convention over another.
   - Emphasizes that the improvement may not be practically significant.

8. **Masking Learning Algorithm Weakness:**
   - Warns against the masking effect where the learning algorithm appears better than HLP but may produce worse results in other scenarios.
   - Highlights the importance of considering overall transcript quality.

9. **Suggested Approach: Raise Human Level Performance:**
   - Suggests a practical approach of aiming to raise HLP instead of solely trying to beat it.
   - Proposes improving label consistency as a means to enhance learning algorithm performance.

10. **Focus on Useful Application Building:**
    - Advocates for prioritizing the development of useful applications over attempting to outperform HLP for academic purposes.
    - Highlights the correlation between improved label consistency and enhanced learning outcomes performance.

**Conclusion:**
Understanding and effectively utilizing Human Level Performance involves recognizing its role as a baseline, acknowledging its limitations, and strategically working towards improving label consistency for more meaningful and practical machine learning applications.

**Title: Understanding Human-Level Performance in Machine Learning**

1. **Introduction:**
   - HLP (Human-Level Performance) in machine learning has gained popularity, sometimes driven by the desire to surpass it for academic recognition.

2. **Challenges in Misuse:**
   - HLP can be misused when the goal is application development rather than academic publication.
   - Ground truth definition plays a crucial role in the relevance of HLP.

3. **Externally Defined Ground Truth:**
   - In cases like medical imaging, where the ground truth is defined by external measures like biopsies, HLP is valuable for comparing algorithms against human predictions.

4. **Issues with Human-Defined Ground Truth:**
   - Challenges arise when the ground truth is labeled by humans, leading to discrepancies and potential ambiguities.

5. **Visual Inspection Example:**
   - Visual inspection instances may exhibit discrepancies between human inspectors, highlighting the need for consistent labeling.

6. **Improving Label Consistency:**
   - Raising HLP by improving label consistency might make it harder for algorithms to surpass HLP but results in cleaner, more consistent data.

7. **Data-Centric Approach:**
   - Advocates for a data-centric approach, emphasizing the importance of high-quality data.

8. **Labeling Ambiguities:**
   - Ambiguous labeling conventions, as seen in visual inspection and speech recognition, contribute to lower HLP.

9. **Structured Data and HLP:**
   - HLP is less frequently used in structured data, but exceptions exist, such as human-labeled data for network traffic or fraud detection.

10. **Consistency in Human Labels:**
    - Questions the meaning of HLP when predicting human-labeled ground truth and stresses the importance of consistent labeling.

11. **Conclusion:**
    - HLP is essential for referencing human performance in relevant applications.
    - Inconsistent labeling instructions impacting HLP should prompt efforts to improve labeling consistency.
    - The goal is not just to beat HLP but to ensure cleaner data for improved machine learning algorithm performance.

**Title: Best Practices for Obtaining Data in Machine Learning Projects**

**1. Introduction:**
   - Overview of the importance of obtaining data for machine learning projects.
   - Emphasis on the iterative nature of the machine learning process.

**2. Time Investment in Data Collection:**
   - Consideration of how much time to spend obtaining data in the overall iterative process.
   - Encouragement to avoid spending excessive time collecting data upfront, hindering the iterative loop.

**3. Swift Entry into Iteration Loop:**
   - Urging teams to quickly enter the iteration loop by setting a time limit for initial data collection (e.g., two days).
   - Highlighting the efficiency gained by starting the training and error analysis phases promptly.

**4. Exception for Previous Problem Experience:**
   - Exceptional case: If prior experience indicates a specific training set size requirement, consider investing more time upfront.

**5. Inventory of Data Sources:**
   - The importance of taking inventory of potential data sources.
   - Using speech recognition as an example, brainstorming various sources, including owned data, crowdsourcing, transcription, and commercial data.

**6. Decision-Making with Trade-offs:**
   - Emphasizing the need to consider trade-offs, including financial costs, time costs, and data quality, when choosing data sources.
   - The significance of adding a column for time costs to better inform decision-making.

**7. Practical Example: Data Labeling:**
   - Three common ways to label data: in-house, outsource, and crowdsource.
   - Mention of trade-offs, such as machine learning engineers labeling being expensive but beneficial for intuition building.

**8. Specialized Labeling Considerations:**
   - Application-dependent factors influencing data labeling, including data quality, privacy, and regulatory constraints.
   - Identifying individuals or groups qualified to provide accurate labels based on the nature of the task.

**9. Caution on Data Set Size Increase:**
   - Suggestion not to increase the dataset size by more than 10x at a time.
   - Advising teams to gradually increase the dataset size, observe changes, and assess the need for further expansion.

**10. Efficiency in Data Collection:**
    - Summary of tips to enhance efficiency in data collection for machine learning projects.
    - Transition to the importance of building data pipelines for managing the flow of data in subsequent videos.




**Title: Managing Replicability in Data Pipelines**

1. **Introduction:**
   - Data pipelines, also known as Data Cascades, involve multiple processing steps before obtaining the final output.
   - Best practices are crucial for effectively managing such data pipelines.

2. **Example Scenario: Predicting Job Search:**
   - Objective: Predict if a user is currently looking for a job based on their information.
   - Raw data requires pre-processing or cleaning before being fed into a learning algorithm for prediction.

3. **Data Cleaning Processes:**
   - Common data cleaning tasks include spam cleanup and user ID merge.
   - In this example, spam cleanup and user ID merge are performed using explicit scripting instructions.

4. **Complexity with Machine Learning Algorithms:**
   - Data cleaning systems could involve machine learning algorithms, adding complexity.
   - Replicability becomes a challenge when transitioning these systems to production deployment.

5. **Challenges with Replicability:**
   - Development Phase: Inconsistencies in pre-processing scripts during the development phase.
   - Scripts may be distributed across different team members' computers, leading to potential discrepancies.

6. **Replicability Effort Dependence:**
   - The level of effort for ensuring replicability depends on the project phase.
   - Proof of Concept (POC) Phase: Focus on getting the prototype to work; some manual pre-processing may be acceptable.
   - Production Phase: High replicability becomes crucial, and sophisticated tools are recommended.

7. **Guidance for Proof of Concept Phase:**
   - During POC, the primary goal is to determine if the application is viable for further development.
   - Acceptable to have some manual data pre-processing, but extensive notes and comments are recommended.

8. **Transition to Production:**
   - When the decision is made to move to production, replicating pre-processing steps becomes essential.
   - Utilize more advanced tools for ensuring the replicability of the entire data pipeline.

9. **Recommended Tools for Replicability:**
   - TensorFlow Transform, Apache Beam, Airflow, etc., are valuable tools for managing replicability.
   - Introduction to TensorFlow Transform covered later in the specialization.

10. **Conclusion:**
    - Importance of replicability in data pipelines.
    - Acknowledgment of more complex data pipelines in various applications.
    - Next video will explore metadata, data provenance, and lineage in the context of complex data pipelines.
    - Replicability, in the context of data pipelines or machine learning projects, refers to the ability to reproduce and recreate the processes and results consistently. It involves ensuring that the steps taken during the development or training phase can be precisely replicated when the system is moved to a different environment or taken into production. The goal is to maintain consistency between the development data and the production data, ensuring that the same processing steps are applied.

**Lecture Title: Managing Metadata and Data Provenance in MLOps**

**Introduction:**
- Metadata, data provenance, and data lineage are crucial for certain applications in MLOps.
- Andrew Ng illustrates their significance using a complex example of a data pipeline.

**Data Pipeline Complexity:**
- Example: Predicting job seekers using user records.
- Involves spam dataset, anti-spam model, user data cleanup, ID merge model, and job prediction model.
- Large commercial systems often have even more complex data pipelines.

**Challenges in Data Pipeline Maintenance:**
- Scenario: IP address blacklists have mistakes, requiring updates.
- Dilemma: Changing upstream data affects multiple interconnected systems.
- Difficulty in fixing issues, especially in systems developed by different engineers.

**Importance of Data Provenance and Lineage:**
- Data Provenance: Where the data came from.
- Lineage: Sequence of steps in the pipeline.
- Challenge: Maintaining a complex system when upstream data changes.

**Tools and Documentation:**
- Documentation is essential for reconstructing data provenance and lineage.
- Tools like TensorFlow Transform aid in tracking changes but are still evolving.
- Calls for more sophisticated tools for managing data pipelines in MLOps.

**Metadata for Improved System Management:**
- Metadata: Data about data.
- Example: In manufacturing visual inspection, metadata includes time, factory, line number, camera settings, phone ID, and inspector ID.
- Metadata aids in error analysis and system improvement.

**Metadata Tips:**
- Extensive use of metadata is recommended.
- Suggests using MLOps tools for storing metadata if available.
- Storing metadata can lead to key insights during machine learning development.

**Speech Recognition Example:**
- Metadata in speech recognition includes the version number of the voice activity detection model.
- Helps in discovering unexpected effects or categories with poor performance.

**Conclusion:**
- Metadata is valuable for error analysis, improving system performance, and tracking data provenance.
- Managing data provenance and lineage is essential for maintaining large and complex machine learning systems.
- The importance of balanced train-dev-test splits will be discussed in the next video.

### Lecture Title: Small Dataset Strategies: Importance of Balanced Train, Dev, Test Split

1. **Introduction:**
   - Andrew Ng emphasizes the significance of data split in small datasets.
   - Common practice: Randomly split data into train, dev, and test sets.

2. **Challenges in Random Split for Small Datasets:**
   - Illustration using a manufacturing visual inspection example with 100 images.
   - Training set: 30 positive (defective phones) and 70 non-defective.
   - Risk of non-representative splits due to chance in a small dataset.

3. **Impact of Unbalanced Split:**
   - Example: Random split leading to 35% positive in training, 10% in dev, and 35% in the test set.
   - Undesirable consequence: Dev set not representative of the true data distribution.

4. **Importance of a Balanced Split:**
   - Ideal scenario: Train, dev, and test sets each having exactly 30% positive examples.
   - Term introduced: "Balanced Split" for more representative datasets.

5. **Achieving a Balanced Split:**
   - Andrew Ng suggests aiming for an exact percentage of positive examples in each set.
   - Enhances reliability in measuring learning algorithm performance.

6. **Size Matters:**
   - No significant concern with large datasets; random split tends to be representative.
   - Focus on balanced split crucial for small datasets (20 dev and 20 test set examples).

7. **Practical Consideration:**
   - Encouragement to use a balanced train, dev, test split in small data problems.
   - Emphasizes its impact on algorithm performance in such scenarios.

8. **Conclusion:**
   - Acknowledges completion of the data section in the course.
   - Optional section on scoping introduced for those interested in project selection.
   - Congratulations on finishing all required videos, hoping the ideas prove beneficial in machine learning projects.

