Week-3
# **Title: Week 3 - Data in Machine Learning Operations (MLOps) Specialization**

- **Introduction:**
  - Final week of the course, focusing on data.
  - Importance of setting up data for training and modeling success.

- **Challenges in Defining Data:**
  - Example of detecting Iguanas to illustrate challenges.
  - Different labeling conventions by diligent labelers.
  - Emphasizes the need for label consistency.

- **Labeling Ambiguity:**
  - Phone defect detection example.
  - Varying opinions on significant defects.
  - Highlight on inconsistent labeling and its impact on learning algorithms.

- **Objective of the Week:**
  - Dive into best practices for the data stage in the machine learning project cycle.
  - Define data components (x and y), establish baselines, and organize data effectively.

- **Data Quality Impact:**
  - Preparation of datasets influences machine learning project success.
  - Importance of well-prepared data sets.

- **Downloading vs. Preparing Data:**
  - Acknowledges the common practice of downloading data for experimentation.
  - Emphasizes the significant impact of data preparation on project success.

- **Upcoming Focus:**
  - Introduction to upcoming examples of ambiguous data.
  - Preparation for techniques to improve data quality.

- **Conclusion:**
  - Setting the stage for exploring data quality improvement techniques in the next video.

**Note:** Understanding the challenges of labeling ambiguity and the impact of inconsistent labeling, the lecture emphasizes the importance of preparing datasets effectively for successful machine learning projects.

# **Title: Understanding Types of Machine Learning Projects**

**Andrew Ng's Lecture Notes: MLOps Specialization Course**

**Introduction:**
- Framework for categorizing machine learning projects based on key factors.

**Two-Axis Categorization:**
- Two axes for categorization: Unstructured Data vs. Structured Data, and Small Data vs. Big Data.
- Unstructured data includes images, audio, and text; humans excel in processing.
- Structured data involves database records, less intuitive for humans.

**Data Size Threshold:**
- Threshold for small and large datasets set at 10,000 examples.
- Transitions are gradual, with emphasis on the manageability of examining examples manually.

**Examples:**
1. **Manufacturing Visual Inspection:**
   - Unstructured data (images of smartphones).
   - Small dataset (100 examples).
   - Data augmentation can help by generating more images.

2. **Predicting Housing Prices:**
   - Structured data (real value features of houses).
   - Small dataset (52 examples).

3. **Speech Recognition:**
   - Unstructured data (audio).
   - Large dataset (50 million examples).
   - Data augmentation useful for synthesizing audio clips.

4. **Product Recommendations:**
   - Structured data (user database).
   - Large dataset (million users).
   - Challenges in obtaining more data and using data augmentation.

**Best Practices for Unstructured Data:**
- Data augmentation is valuable (e.g., for image or speech data).
- Human labeling and synthesizing new data enhance performance.

**Best Practices for Structured Data:**
- Challenges in obtaining more data.
- Data augmentation is harder.
- Human labeling may be challenging due to limited examples.

**Data Size Considerations:**
- Small datasets require clean labels; inconsistencies impact a significant portion.
- Large datasets emphasize data processes; human review may be impractical.

**Data Size and Data Processes:**
- Small datasets benefit from manual review and consistent labeling.
- Large datasets require robust data processes due to the scale of labeling.

**Generalization and Hiring Insights:**
- Categorization aids in predicting generalization of data processes and ML ideas.
- Hiring individuals with experience in the same quadrant is advantageous.

**Adapting Advice:**
- Caution against one-size-fits-all advice in machine learning.
- Diverse problems may require different approaches; seek advice from the relevant quadrant.

**Conclusion:**
- Understanding the categorization of machine learning problems aids in tailored solutions.
- Importance of clean data, especially in small data problems.
- Next video: Exploring why clean data is crucial in small data problems.


# **Title: Handling Small Datasets and Label Consistency**

**Introduction:**
- Addressing challenges in small datasets and the importance of clean, consistent labels.

**Small Datasets and Noisy Labels:**
- Small datasets, especially with noisy labels, pose challenges in confidently fitting a function.
- Illustration using a helicopter rotor speed prediction problem with only five examples.
- Difficulty in determining the mapping function from voltage to rotor speed with a small, noisy dataset.

**Learning from Large Datasets:**
- Contrast with large datasets where algorithms can average over noise, enabling more confident function fitting.
- Caution against neglecting practices for small datasets prevalent in large consumer Internet companies.

**Clean Labels in Small Datasets:**
- Emphasis on the significance of clean and consistent labels in small datasets.
- Example of training computer vision systems with just 30 images, highlighting the importance of clean labels.

**Phone Defect Inspection Example:**
- Discussing phone defect inspection and the impact of unclear labeling instructions on label inconsistency.
- Proposal to improve label consistency by having inspectors agree on scratch size thresholds.
- Importance of defining tasks, like bounding boxes around defects, to enhance image labeling consistency.

**Label Consistency Enhances Accuracy:**
- Demonstrating how label consistency leads to higher accuracy even with a limited dataset.
- Importance of inspectors agreeing on criteria for defects, facilitating more consistent image labeling.

**Small Data Challenges in Big Data Problems:**
- Acknowledgment that big data problems can still have small data challenges, particularly with rare events.
- Examples include rare web search queries, critical events in self-driving cars, and long-tail items in product recommendation systems.
- Highlighting the need for label consistency even in large datasets where rare events have limited examples.

**Conclusion:**
- Reinforcing the critical role of label consistency in small datasets and its ongoing importance in big data scenarios.
- Teaser for the next video focusing on concrete ideas and best practices for improving data label consistency.

# **Title: Improving Label Consistency in Machine Learning Datasets**

1. **Process for Label Consistency:**
   - If concerned about inconsistent labels, have multiple labelers label the same examples.
   - Consider having the same labeler re-label an example after a break to check for self-consistency.
   - Address disagreements by involving responsible individuals like machine label engineers or subject matter experts.
   - Discuss and document a more consistent definition of the label (y) and update labeling instructions.

2. **Adjusting Input Information (x):**
   - If labelers find input (x) lacking information, consider modifying the input, such as improving lighting for images.
   - Iterative process: After improving x or label instructions, ask the team to label more data.

3. **Standardizing Definitions:**
   - Common outcome is standardizing label definitions to enhance data consistency.
   - Example: Standardizing the convention for labeling audio clips.

4. **Merging Classes:**
   - In cases of unclear distinctions, merge classes to simplify the task for learning algorithms.
   - Example: Merging classes for deep and shallow scratches on a phone surface.

5. **Creating a New Class for Uncertainty:**
   - Introduce a new class or label to capture uncertainty when labelers struggle to agree.
   - Example: Creating a borderline class for ambiguous cases in defect labeling.

6. **Speech Recognition Example:**
   - Utilize the example of unintelligible speech to introduce a new tag (unintelligible) for ambiguous cases.
   - Enhances labeling consistency when the audio clip is genuinely unclear.

7. **Working with Small vs. Big Datasets:**
   - For small datasets, resolve inconsistencies through discussions with labelers.
   - For big datasets, define consistent labels with a small group before instructing a larger group of labelers.

8. **Voting Mechanism (Consensus Labeling):**
   - Overused technique: Having multiple labelers vote to resolve inconsistencies.
   - Prefer addressing inconsistent instructions first to make individual labeler choices less noisy.

9. **Gaps in the ML World:**
   - Acknowledges the need for tools and ML Ops tools to enhance the consistency and repeatability of label improvement processes.
   - Calls for the development of tools to detect label inconsistencies and facilitate data quality improvement.

10. **Human Level Performance:**
    - Raises the question of human level performance on the task.
    - Highlights the importance and sometimes misuse of human level performance in machine learning.

11. **Looking Forward:**
    - Expresses anticipation for community efforts in developing tools to detect and improve label inconsistencies consistently.
    - Emphasizes the importance of addressing gaps in current ML practices for enhancing label quality.
# **Title: Understanding Human Level Performance in Machine Learning**

**Key Points:**
1. **Introduction to Human Level Performance (HLP):**
   - HLP serves as a reference baseline for predicting inherently ambiguous outputs in certain machine learning tasks.
   - Sometimes misused, especially when dealing with unstructured data tasks.

2. **Estimating Irreducible Error:**
   - HLP is crucial for estimating Bayes error or irreducible error, particularly in unstructured data tasks.
   - Aids in analysis, prioritization, and establishing the potential of a given task.

3. **Example of Visual Inspection Tasks:**
   - Illustration using a visual inspection task to demonstrate the practical application of HLP.
   - Highlights the importance of understanding ground truth labels.

4. **Academic Benchmarking:**
   - In academia, HLP serves as a benchmark for research significance.
   - Beating HLP often contributes to establishing the academic significance of machine learning work.

5. **Caution Against Misuse of HLP:**
   - Cautionary note against using HLP to assert machine superiority over humans in practical applications.
   - Highlights the limitations, especially in scenarios where applications require more than just high accuracy.

6. **Inconsistent Labeling Instructions Issue:**
   - Discusses the problem of inconsistent labeling instructions affecting HLP measurements.
   - Demonstrates how a learning algorithm may gain an unfair advantage due to such inconsistencies.

7. **Unfair Advantage Scenario:**
   - Explains the scenario where a learning algorithm gains an advantage by consistently choosing one labeling convention over another.
   - Emphasizes that the improvement may not be practically significant.

8. **Masking Learning Algorithm Weakness:**
   - Warns against the masking effect where the learning algorithm appears better than HLP but may produce worse results in other scenarios.
   - Highlights the importance of considering overall transcript quality.

9. **Suggested Approach: Raise Human Level Performance:**
   - Suggests a practical approach of aiming to raise HLP instead of solely trying to beat it.
   - Proposes improving label consistency as a means to enhance learning algorithm performance.

10. **Focus on Useful Application Building:**
    - Advocates for prioritizing the development of useful applications over attempting to outperform HLP for academic purposes.
    - Highlights the correlation between improved label consistency and enhanced learning outcomes performance.

**Conclusion:**
Understanding and effectively utilizing Human Level Performance involves recognizing its role as a baseline, acknowledging its limitations, and strategically working towards improving label consistency for more meaningful and practical machine learning applications.

# **Title: Understanding Human-Level Performance in Machine Learning**

1. **Introduction:**
   - HLP (Human-Level Performance) in machine learning has gained popularity, sometimes driven by the desire to surpass it for academic recognition.

2. **Challenges in Misuse:**
   - HLP can be misused when the goal is application development rather than academic publication.
   - Ground truth definition plays a crucial role in the relevance of HLP.

3. **Externally Defined Ground Truth:**
   - In cases like medical imaging, where the ground truth is defined by external measures like biopsies, HLP is valuable for comparing algorithms against human predictions.

4. **Issues with Human-Defined Ground Truth:**
   - Challenges arise when the ground truth is labeled by humans, leading to discrepancies and potential ambiguities.

5. **Visual Inspection Example:**
   - Visual inspection instances may exhibit discrepancies between human inspectors, highlighting the need for consistent labeling.

6. **Improving Label Consistency:**
   - Raising HLP by improving label consistency might make it harder for algorithms to surpass HLP but results in cleaner, more consistent data.

7. **Data-Centric Approach:**
   - Advocates for a data-centric approach, emphasizing the importance of high-quality data.

8. **Labeling Ambiguities:**
   - Ambiguous labeling conventions, as seen in visual inspection and speech recognition, contribute to lower HLP.

9. **Structured Data and HLP:**
   - HLP is less frequently used in structured data, but exceptions exist, such as human-labeled data for network traffic or fraud detection.

10. **Consistency in Human Labels:**
    - Questions the meaning of HLP when predicting human-labeled ground truth and stresses the importance of consistent labeling.

11. **Conclusion:**
    - HLP is essential for referencing human performance in relevant applications.
    - Inconsistent labeling instructions impacting HLP should prompt efforts to improve labeling consistency.
    - The goal is not just to beat HLP but to ensure cleaner data for improved machine learning algorithm performance.

